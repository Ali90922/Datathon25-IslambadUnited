# trained_model.py

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression  # or another model of your choice
import joblib

def train_and_save_model():
    # 1. Load the dataset
    df = pd.read_csv("Substance_Use_20250301.csv")

    # 2. Example: Minimal data cleaning
    #    Let's assume we want to predict how often a certain substance appears or
    #    the number of calls in each 'Ward' or 'Neighbourhood'.
    #    This heavily depends on your real use case.

    # Example: Suppose we want to predict 'Age' as a numeric variable
    # (this is just a contrived exampleâ€”replace with your actual prediction goal!)
    # Let's make the 'Age' column numeric by taking the middle of the range, e.g. "30 to 34" => 32
    # This is purely for demonstration:
    
    def parse_age(age_str):
        # age_str might be something like "30 to 34"
        # We'll split and take the average
        try:
            parts = age_str.split(" to ")
            low = int(parts[0])
            high = int(parts[1])
            return (low + high) // 2  # integer average
        except:
            return None  # handle if it's malformed

    df["AgeNumeric"] = df["Age"].apply(parse_age)
    df = df.dropna(subset=["AgeNumeric"])  # drop rows where Age couldn't be parsed

    # Similarly, let's encode "Gender" as numeric (0 = Female, 1 = Male) for demonstration
    df["GenderNum"] = df["Gender"].apply(lambda x: 1 if x.strip().lower() == "male" else 0)

    # We also have columns like "Substance", "Ward", etc. We can do one-hot encoding if needed:
    # For brevity, let's just pick a few numeric columns to feed a simple linear model.
    # (In real life, you'd do more advanced feature engineering.)

    features = ["GenderNum", "AgeNumeric"]
    df = df.dropna(subset=features)  # ensure no missing

    X = df[features]
    # Let's pretend we want to predict if the "Substance" is "Opioids" or not (binary).
    # We'll create a target column "IsOpioid" for classification. 
    # If you truly want regression on a numeric target, adapt accordingly.

    df["IsOpioid"] = df["Substance"].apply(lambda s: 1 if s.strip().lower() == "opioids" else 0)

    y = df["IsOpioid"]  # classification target (0 or 1)

    # 3. Train-Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )

    # 4. Train a (simple) linear model (though logistic regression is more appropriate for classification)
    model = LinearRegression()
    model.fit(X_train, y_train)

    # 5. Evaluate quickly (for demonstration)
    score = model.score(X_test, y_test)  # R^2 for linear regression
    print("Model R^2 on test set:", score)

    # 6. Save the model
    joblib.dump(model, "model.pkl")
    print("Model saved to model.pkl")

if __name__ == "__main__":
    train_and_save_model()

